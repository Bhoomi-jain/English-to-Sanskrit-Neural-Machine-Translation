{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50f04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f746c3",
   "metadata": {},
   "source": [
    "Path → safe file handling   \n",
    "random → data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cc970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW = Path(\"data/raw\")     #original untouched data(read-only)\n",
    "DATA_PROCESSED = Path(\"data/processed\")     #cleaned + merged data will go(model reads from here)\n",
    "TOKENIZER_DIR = Path(\"tokenizer\")       #SentencePiece models will be saved here\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)       #Creates folders only if they don’t exist\n",
    "TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5265ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPCC\n",
    "bpcc_en = (DATA_RAW / \"bpcc/train.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "bpcc_sa = (DATA_RAW / \"bpcc/train.sa\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "# Samayik\n",
    "sam_train_en = (DATA_RAW / \"samayik/train.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "sam_train_sa = (DATA_RAW / \"samayik/train.sa\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "sam_dev_en = (DATA_RAW / \"samayik/dev.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "sam_dev_sa = (DATA_RAW / \"samayik/dev.sa\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "sam_test_en = (DATA_RAW / \"samayik/test.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "sam_test_sa = (DATA_RAW / \"samayik/test.sa\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "# Load Itihasa (parallel, aligned)\n",
    "iti_train_en = (DATA_RAW / \"itihasa/train.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "iti_train_sa = (DATA_RAW / \"itihasa/train.sn\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "iti_dev_en = (DATA_RAW / \"itihasa/dev.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "iti_dev_sa = (DATA_RAW / \"itihasa/dev.sn\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "iti_test_en = (DATA_RAW / \"itihasa/test.en\").read_text(encoding=\"utf-8\").splitlines()\n",
    "iti_test_sa = (DATA_RAW / \"itihasa/test.sn\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a59aef",
   "metadata": {},
   "source": [
    "Each file is read as UTF-8 (mandatory for Devanagari), split by newline, becomes List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b04e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPCC: 98788 99424\n",
      "Samayik train: 43493 43493\n",
      "Samayik dev: 2416 2416\n",
      "Samayik test: 2417 2417\n",
      "Itihasa train: 75161 75161\n",
      "Itihasa dev: 6148 6148\n",
      "Itihasa test: 11721 11721\n"
     ]
    }
   ],
   "source": [
    "print(\"BPCC:\", len(bpcc_en), len(bpcc_sa))\n",
    "\n",
    "print(\"Samayik train:\", len(sam_train_en), len(sam_train_sa))\n",
    "print(\"Samayik dev:\", len(sam_dev_en), len(sam_dev_sa))\n",
    "print(\"Samayik test:\", len(sam_test_en), len(sam_test_sa))\n",
    "\n",
    "print(\"Itihasa train:\", len(iti_train_en), len(iti_train_sa))\n",
    "print(\"Itihasa dev:\", len(iti_dev_en), len(iti_dev_sa))\n",
    "print(\"Itihasa test:\", len(iti_test_en), len(iti_test_sa))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9932a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was no Mughal tradition of primogeniture, the systematic passing of rule, upon an emperor's death, to his eldest son.\n",
      "चक्रवर्तिनः मृत्योः अनन्तरं तस्य शासनस्य व्यवस्थितरूपेण सङ्क्रमणस्य, मुघलपरम्परायाः ज्येष्ठपुत्राधिकारपद्धतिः नासीत्।\n"
     ]
    }
   ],
   "source": [
    "print(bpcc_en[0])\n",
    "print(bpcc_sa[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca28e03",
   "metadata": {},
   "source": [
    "BPCC data is mismatched and not aligned, we'll use it for tokenizer training, not for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25513f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: O Rishi, then they obtain high fortune and happiness. If one however cannot acquire knowledge, he takes an inferior birth. The fruits of acts performed in this world are reaped in the next. O Brahmana, this worid has been declared to be one of acts.\n",
      "SA: तत्रापि स महाभागः सुखभागभिजायते। न चेत् सम्बुध्यते तत्र गच्छत्यधमतां ततः॥ इह यत् क्रियते कर्म तत् परत्रोपभुज्यते। कर्मभूमिरियं ब्रह्मन् फलभूमिरसौ मता॥\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(iti_train_en) - 1)\n",
    "print(\"EN:\", iti_train_en[i])\n",
    "print(\"SA:\", iti_train_sa[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e33952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: \"\"\"For if thou wert cut out of the olive tree which is wild by nature, and wert graffed contrary to nature into a good olive tree: how much more shall these, which be the natural branches, be graffed into their own olive tree?\"\"\"\n",
      "SA: वन्यजितवृक्षस्य शाखा सन् त्वं यदि ततश्छिन्नो रीतिव्यत्ययेनोत्तमजितवृक्षे रोेेपितोऽभवस्तर्हि तस्य वृक्षस्य स्वीया याः शाखास्ताः किं पुनः स्ववृक्षे संलगितुं न शक्नुवन्ति?\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(sam_train_en) - 1)\n",
    "print(\"EN:\", sam_train_en[i])\n",
    "print(\"SA:\", sam_train_sa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a9afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final TRAIN size: 118654\n"
     ]
    }
   ],
   "source": [
    "# Translation training corpus\n",
    "train_en = sam_train_en + iti_train_en\n",
    "train_sa = sam_train_sa + iti_train_sa\n",
    "\n",
    "print(\"Final TRAIN size:\", len(train_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6702e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer corpus:\n",
      "EN sentences: 217442\n",
      "SA sentences: 218078\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer corpus \n",
    "tok_en = bpcc_en + train_en\n",
    "tok_sa = bpcc_sa + train_sa\n",
    "\n",
    "print(\"Tokenizer corpus:\")\n",
    "print(\"EN sentences:\", len(tok_en))\n",
    "print(\"SA sentences:\", len(tok_sa))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad05e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN sentences: 118654\n",
      "EN min: 1\n",
      "EN max: 1306\n",
      "EN avg: 24.342188211101185\n"
     ]
    }
   ],
   "source": [
    "en_lengths = [len(s.split()) for s in train_en if s.strip()]\n",
    "\n",
    "print(\"EN sentences:\", len(en_lengths))\n",
    "print(\"EN min:\", min(en_lengths))\n",
    "print(\"EN max:\", max(en_lengths))\n",
    "print(\"EN avg:\", sum(en_lengths) / len(en_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b4b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA sentences: 118654\n",
      "SA min: 1\n",
      "SA max: 2688\n",
      "SA avg: 87.45049471572808\n"
     ]
    }
   ],
   "source": [
    "sa_lengths = [len(s) for s in train_sa if s.strip()]\n",
    "\n",
    "print(\"SA sentences:\", len(sa_lengths))\n",
    "print(\"SA min:\", min(sa_lengths))\n",
    "print(\"SA max:\", max(sa_lengths))\n",
    "print(\"SA avg:\", sum(sa_lengths) / len(sa_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c72e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25313780"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(DATA_PROCESSED / \"tok_en.txt\").write_text(\n",
    "    \"\\n\".join(tok_en),\n",
    "    encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c2d2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19646646"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(DATA_PROCESSED / \"tok_sa.txt\").write_text(\n",
    "    \"\\n\".join(tok_sa),\n",
    "    encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35ff476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tokenizer already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Train English tokenizer ONLY if it doesn't exist\n",
    "if not (TOKENIZER_DIR / \"spm_en.model\").exists():\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=str(DATA_PROCESSED / \"tok_en.txt\"),\n",
    "        model_prefix=str(TOKENIZER_DIR / \"spm_en\"),\n",
    "        vocab_size=16000,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        unk_id=3\n",
    "    )\n",
    "else:\n",
    "    print(\"English tokenizer already exists. Skipping training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c29c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanskrit tokenizer already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Train Sanskrit tokenizer ONLY if it doesn't exist\n",
    "if not (TOKENIZER_DIR / \"spm_sa.model\").exists():\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=str(DATA_PROCESSED / \"tok_sa.txt\"),\n",
    "        model_prefix=str(TOKENIZER_DIR / \"spm_sa\"),\n",
    "        vocab_size=32000,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=1.0,\n",
    "        pad_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        unk_id=3\n",
    "    )\n",
    "else:\n",
    "    print(\"Sanskrit tokenizer already exists. Skipping training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf8942",
   "metadata": {},
   "source": [
    "TOKENIZER (RUN ONCE ONLY)\n",
    "DO NOT RETRAIN UNLESS DATA CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "642b15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48ff04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823a40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
